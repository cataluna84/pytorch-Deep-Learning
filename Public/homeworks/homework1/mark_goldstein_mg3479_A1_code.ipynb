{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EyJt4yZIQAem"
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# You can find Alfredo's plotting code in plot_lib.py in this directory .\n",
    "# Download it along with this assignment and keep it in the same directory.\n",
    "from plot_lib import set_default, show_scatterplot, plot_bases\n",
    "\n",
    "from matplotlib.pyplot import plot, title, axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTHDQFHFQAeq"
   },
   "outputs": [],
   "source": [
    "# Set up your device \n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "exqe27QXQAet"
   },
   "outputs": [],
   "source": [
    "# Set up random seed to 1008. Do not change the random seed.\n",
    "# Yes, these are all necessary when you run experiments!\n",
    "seed = 1008\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7krs-4XOQAev"
   },
   "source": [
    "# 1. Full, slice, fill\n",
    "Write a function `warm_up` that returns the 2D tensor with integers below. **Do not use any loops**.\n",
    "\n",
    "```\n",
    "1 2 1 1 1 1 2 1 1 1 1 2 1\n",
    "2 2 2 2 2 2 2 2 2 2 2 2 2\n",
    "1 2 1 1 1 1 2 1 1 1 1 2 1\n",
    "1 2 1 3 3 1 2 1 3 3 1 2 1\n",
    "1 2 1 3 3 1 2 1 3 3 1 2 1\n",
    "1 2 1 1 1 1 2 1 1 1 1 2 1\n",
    "2 2 2 2 2 2 2 2 2 2 2 2 2\n",
    "1 2 1 1 1 1 2 1 1 1 1 2 1\n",
    "1 2 1 3 3 1 2 1 3 3 1 2 1\n",
    "1 2 1 3 3 1 2 1 3 3 1 2 1\n",
    "1 2 1 1 1 1 2 1 1 1 1 2 1\n",
    "2 2 2 2 2 2 2 2 2 2 2 2 2\n",
    "1 2 1 1 1 1 2 1 1 1 1 2 1\n",
    "```\n",
    "\n",
    "\n",
    "Hint: Use `torch.full`, `torch.fill_`, and the slicing operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JsFk9OUoQAew"
   },
   "outputs": [],
   "source": [
    "def warm_up():\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Uncomment line below once you implement this function. \n",
    "# print(warm_up())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E8ivZVl5QAez"
   },
   "source": [
    "# 2. To Loop or not to loop\n",
    "\n",
    "The motivation for the following three sub-questions is to get you to think critically about how to write your deep learning code. These sorts of choices can make the difference between tractable and intractable model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxGRJK4-QAe0"
   },
   "source": [
    "## 2.1. `mul_row_loop`\n",
    "Write a function `mul_row_loop`, using python loops with simple indexing but no advanced indexing/slicing, that receives a 2D tensor as input and returns a tensor of same size that is \n",
    "- equal to the input on the first row\n",
    "- 2 times the input's second row on the second row\n",
    "- 3 times the input's third row on the third row\n",
    "- etc..\n",
    "\n",
    "For instance:\n",
    "```\n",
    ">>> t = torch.full((4, 8), 2.0)\n",
    ">>> t\n",
    "tensor([[2., 2., 2., 2., 2., 2., 2., 2.],\n",
    "[2., 2., 2., 2., 2., 2., 2., 2.],\n",
    "[2., 2., 2., 2., 2., 2., 2., 2.],\n",
    "[2., 2., 2., 2., 2., 2., 2., 2.]])\n",
    ">>> mul_row(t)\n",
    "tensor([[2., 2., 2., 2., 2., 2., 2., 2.],\n",
    "[4., 4., 4., 4., 4., 4., 4., 4.],\n",
    "[6., 6., 6., 6., 6., 6., 6., 6.],\n",
    "[8., 8., 8., 8., 8., 8., 8., 8.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5YIAg1mlQAe0"
   },
   "outputs": [],
   "source": [
    "def mul_row_loop(input_tensor):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ycMZbO2HQAe3"
   },
   "source": [
    "# 2.2. `mul_row_fast`\n",
    "Write a second version of the same function named `mul_row_fast` which uses tensor operations and no looping.\n",
    "\n",
    "**Hint**: Use broadcasting and `torch.arange`, `torch.view`, and `torch.mul`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5V3Q-GX8QAe3"
   },
   "outputs": [],
   "source": [
    "def mul_row_fast(input_tensor):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6MWuVzVQQAe7"
   },
   "source": [
    "# 2.3. `times`\n",
    "Write a function `times` which takes a 2D tensor as input and returns the run times of `mul_row_loop` and `mul_row_fast` on this tensor, respectively. Use `time.perf_counter`.\n",
    "\n",
    "Use `torch.ones` to create a 2D tensor of size (1000, 400) full of ones and run `times` on it (there should be more than two orders of magnitude difference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vlOKGDMJQAe8"
   },
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "def times(input_tensor):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Uncomment lines below once you implement this function. \n",
    "# input_tensor = TODO\n",
    "# time_1, time_2 = times(random_tensor)\n",
    "# print('{}, {}'.format(time_1, time_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yn_FXad8QAe_"
   },
   "source": [
    "# 3. Non-linearities\n",
    "\n",
    "In this section, we explore similar concepts to Lab 1 and get comfortable initializing modules like nn.Linear and using non-linearities in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19a6rMNeQAe_"
   },
   "source": [
    "## 3.1. ReLU\n",
    "ReLU (Rectified Linear Unit) is a non-linear activation fuction defined as:\n",
    "\n",
    "$$y = \\mathrm{max}(0,x)$$\n",
    "\n",
    "Define a fully connected neural network `linear_fc_relu` which:\n",
    "- takes 2 dimensional data as input and passes it through linear modules (`torch.nn.Linear`)\n",
    "- has one hidden layer of dimension 5 \n",
    "- has output dimension of 2\n",
    "- has ReLu as an activation function\n",
    "\n",
    "Create a tensor with input data $X$ of size (100, 2) using `torch.randn`. \n",
    "\n",
    "Following the example in https://github.com/Atcold/pytorch-Deep-Learning-Minicourse/blob/master/02-space_stretching.ipynb, visualize the output of passing `X` through the neural network `linear_fc_relu`.\n",
    "\n",
    "You can find Alfredo's plotting code in plot_lib.py in this directory. Download it along with this assignment and keep it in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XX0oEX98QAfA"
   },
   "outputs": [],
   "source": [
    "# Input data\n",
    "# X = TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zf-DCcGEQAfI"
   },
   "outputs": [],
   "source": [
    "# create 1-layer neural networks with ReLU activation\n",
    "# linear_fc_relu = TODO\n",
    "# Visualize: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RSznMa6rQAfL"
   },
   "source": [
    "## 3.2. Sigmoid\n",
    "The sigmoid function is another popular choice for a non-linear activation function which maps its input to values in the interval $(0,1)$. It is formally defined as:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+ exp[-x]}$$\n",
    "\n",
    "Define a new neural network `linear_fc_sigmoid` which is the same architecture as in part `3.1.` but with a sigmoid unit instead of ReLU. \n",
    "\n",
    "Using the same $X$ as in part `3.1`, visualize the output of passing `X` through the neural network `linear_fc_sigmoid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5xg3uBHNQAfM"
   },
   "outputs": [],
   "source": [
    "# create 1-layer neural networks with Sigmoid activation\n",
    "# linear_fc_sigmoid = TODO\n",
    "# Visualize: TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DS-GA-1008-HW_assignment_1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
